import logging
import json
import re
from typing import Optional

from openai import AsyncOpenAI

from app.config import settings

logger = logging.getLogger(__name__)

_client: AsyncOpenAI | None = None

NEWS_TAGS = [
    "#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    "#LLM",
    "#GPT",
    "#–ê–≥–µ–Ω—Ç—ã",
    "#OpenSourceAI",
    "#–†–µ–ª–∏–∑–ú–æ–¥–µ–ª–∏",
    "#–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏AI",
    "#–†–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µAI",
    "#–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—åAI",
    "#–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
]


def _extract_json_object(text: str) -> dict:
    text = text.strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if not match:
            raise
        return json.loads(match.group(0))


def get_llm_client() -> AsyncOpenAI:
    global _client
    if _client is None:
        _client = AsyncOpenAI(
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url,
        )
    return _client


async def summarize_post(content: str) -> Optional[str]:
    """Generate a concise summary of a post/article using DeepSeek."""
    client = get_llm_client()

    # Truncate very long content
    if len(content) > 4000:
        content = content[:4000] + "..."

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫—Ä–∞—Ç–∫–æ –∏–∑–ª–æ–∂–∏—Ç—å —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ "
                        "–≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –≤—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã. "
                        "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Å–≤–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏."
                    ),
                },
                {
                    "role": "user",
                    "content": f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏:\n\n{content}",
                },
            ],
            max_tokens=300,
            temperature=0.3,
        )
        summary = response.choices[0].message.content.strip()
        logger.debug(f"Summary generated: {summary[:80]}...")
        return summary
    except Exception as e:
        logger.error(f"LLM summarization error: {e}")
        return None


async def analyze_post(content: str) -> dict:
    """
    Single LLM call for summary + AI relevance + CoreAI importance.
    Returns:
      {
        "summary": str,
        "is_relevant": bool,
        "coreai_score": float (0..1),
        "coreai_reason": str,
        "tags": list[str],
        "news_kind": str,
        "implementable_by_small_team": bool,
        "infra_barrier": str,
        "product_score": float,
        "priority": str,
        "is_alert_worthy": bool,
        "analogs": list[str],
        "action_item": str,
      }
    """
    client = get_llm_client()

    if len(content) > 4000:
        content = content[:4000] + "..."

    prompt = (
        "–¢—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π AI-–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã CoreAI.\n"
        "–¢–≤–æ—è —Ü–µ–ª—å: –æ—Ç–±–∏—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ —É–ª—É—á—à–∞—Ç—å AI/LLM-–ø—Ä–æ–¥—É–∫—Ç—ã.\n"
        "–§–û–ö–£–°: —Ä–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã —É—Å–ø–µ—Ö–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (—Ç—Ä–∞—Ñ–∏–∫, –≤—ã—Ä—É—á–∫–∞, —ç–∫–∑–∏—Ç, –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ —Å —Ü–∏—Ñ—Ä–∞–º–∏). –ê–Ω–æ–Ω—Å—ã –≤–µ–Ω–¥–æ—Ä–æ–≤ (–º–æ–¥–µ–ª–∏, API, –∞–≥–µ–Ω—Ç—ã) ‚Äî —ç—Ç–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –Ω–µ product.\n\n"
        "–í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –°–¢–†–û–ì–û –≤ JSON —Å –ø–æ–ª—è–º–∏:\n"
        "summary: string (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º, —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã)\n"
        "is_relevant: boolean\n"
        "coreai_score: number –æ—Ç 0 –¥–æ 1\n"
        "coreai_reason: string (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ/–Ω–µ –≤–∞–∂–Ω–æ –¥–ª—è CoreAI)\n"
        "analogs: array<string> (0-3 –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤/–∞–Ω–∞–ª–æ–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–æ–∏—Ç –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å)\n"
        "action_item: string (–æ–¥–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è CoreAI, —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –≤ –ø–æ–≤–µ–ª–∏—Ç–µ–ª—å–Ω–æ–º —Å—Ç–∏–ª–µ)\n"
        "tags: array<string> –∏–∑ 1-3 —Ö–µ—à—Ç–µ–≥–æ–≤\n"
        "news_kind: string –∏–∑ {product, trend, research, tech_update, industry_report, misc}\n"
        "implementable_by_small_team: boolean\n"
        "infra_barrier: string –∏–∑ {low, medium, high}\n"
        "product_score: number –æ—Ç 0 –¥–æ 1\n"
        "priority: string –∏–∑ {high, medium, low}\n"
        "is_alert_worthy: boolean\n\n"
        f"–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —ç—Ç–∏ —Ö–µ—à—Ç–µ–≥–∏: {', '.join(NEWS_TAGS)}\n\n"
        "–ü—Ä–∞–≤–∏–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π):\n"
        "- product: –¢–û–õ–¨–ö–û –∫–µ–π—Å—ã —É—Å–ø–µ—Ö–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏ ‚Äî –∫—Ç–æ-—Ç–æ —Å–æ–∑–¥–∞–ª –ø—Ä–æ–¥—É–∫—Ç/—Å–µ—Ä–≤–∏—Å –∏ –µ—Å—Ç—å –¥–æ–∫–∞–∑–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ü—Ä–∏–º–µ—Ä—ã: ¬´—Å—Ç–∞—Ä—Ç–∞–ø –∑–∞ –Ω–µ–¥–µ–ª—é –Ω–∞–±—Ä–∞–ª N –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π¬ª, ¬´—Å–µ—Ä–≤–∏—Å –∫—É–ø–∏–ª–∏ (Google/–¥—Ä—É–≥–∞—è –∫–æ–º–ø–∞–Ω–∏—è)¬ª, ¬´–≤–Ω–µ–¥—Ä–∏–ª–∏ –∏ —Å—ç–∫–æ–Ω–æ–º–∏–ª–∏ X% / Y –º–ª–Ω¬ª, ¬´–≤—ã—Ä—É—á–∫–∞ Z¬ª, ¬´N –∫–æ–º–ø–∞–Ω–∏–π —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç¬ª. –ë–µ–∑ —Ü–∏—Ñ—Ä —É—Å–ø–µ—Ö–∞/—Ç—Ä–∞—Ñ–∏–∫–∞/—ç–∫–∑–∏—Ç–∞ ‚Äî –ù–ï product. –õ—é–±–æ–π –∞–Ω–æ–Ω—Å –≤–∏–¥–∞ ¬´–∫–æ–º–ø–∞–Ω–∏—è X –≤—ã–ø—É—Å—Ç–∏–ª–∞ –º–æ–¥–µ–ª—å/–∞–≥–µ–Ω—Ç–∞/API¬ª ‚Äî —ç—Ç–æ tech_update, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ ¬´–∑–∞–ø—É—Å—Ç–∏–ª–∏ –≤ preview¬ª –∏–ª–∏ ¬´–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º¬ª.\n"
        "- tech_update: –≤—Å—ë, —á—Ç–æ ¬´–≤—ã—à–ª–æ¬ª –∏–ª–∏ ¬´–º–æ–∂–Ω–æ —é–∑–∞—Ç—å¬ª, –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞ –ø—Ä–æ–¥—É–∫—Ç–∞. –í–°–ï–ì–î–ê —Å—é–¥–∞: —Ä–µ–ª–∏–∑—ã –º–æ–¥–µ–ª–µ–π (GLM-4.7, Gemini, GPT, Claude, Llama), –∞–Ω–æ–Ω—Å—ã –∞–≥–µ–Ω—Ç–æ–≤ (Claude Cowork –∏ —Ç.–¥.), –Ω–æ–≤—ã–µ API, preview –æ—Ç –≤–µ–Ω–¥–æ—Ä–æ–≤, –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫/SDK, –≥–∞–π–¥—ã, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è, best practices, licensing/pricing. –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ ¬´X –≤—ã–ø—É—Å—Ç–∏–ª–∞ Y¬ª, ¬´–∑–∞–ø—É—Å—Ç–∏–ª–∏ –≤ preview¬ª, ¬´–¥–æ—Å—Ç—É–ø–µ–Ω –≤ API¬ª ‚Äî tech_update, –Ω–µ product.\n"
        "- trend: –∫—Ä—É–ø–Ω—ã–π —Ä—ã–Ω–æ—á–Ω—ã–π —Å–¥–≤–∏–≥, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–æ–¥—É–∫—Ç–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.\n"
        "- research: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤ –ø—Ä–æ–¥—É–∫—Ç–µ –≤ –æ–±–æ–∑—Ä–∏–º–æ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ.\n"
        "- industry_report: –æ—Ç—á–µ—Ç—ã McKinsey/BCG/Gartner/Deloitte/—Ñ–æ–Ω–¥–æ–≤ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏.\n"
        "- misc: –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ.\n\n"
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏ implementable_by_small_team=true:\n"
        "- —Ä–µ—à–µ–Ω–∏–µ –º–æ–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∫–æ–º–∞–Ω–¥–æ–π 3-7 —á–µ–ª–æ–≤–µ–∫ –∑–∞ 2-8 –Ω–µ–¥–µ–ª—å;\n"
        "- –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ frontier-–º–æ–¥–µ–ª–µ–π, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞-—Ü–µ–Ω—Ç—Ä—ã –∏ –º–Ω–æ–≥–æ–º–∏–ª–ª–∏–æ–Ω–Ω—ã–π capex;\n"
        "- –æ–ø–æ—Ä–∞ –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ã–µ API/opensource/—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π cloud stack.\n\n"
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏ infra_barrier:\n"
        "- low: –º–æ–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö API/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤;\n"
        "- medium: —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–ª–æ–∂–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è MLOps;\n"
        "- high: –Ω—É–∂–Ω–∞ —Ç—è–∂–µ–ª–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫—Ä—É–ø–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª.\n\n"
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏ high priority:\n"
        "- –¥–ª—è product: —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ü–∏—Ñ—Ä —É—Å–ø–µ—Ö–∞ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –≤—ã—Ä—É—á–∫–∞, —ç–∫–∑–∏—Ç, –∫–µ–π—Å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏);\n"
        "- –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ –∫–µ–π—Å—ã –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Å –≤–ª–∏—è–Ω–∏–µ–º –Ω–∞ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏ (–≤—ã—Ä—É—á–∫–∞, –∫–æ–Ω–≤–µ—Ä—Å–∏—è, CAC, —É–¥–µ—Ä–∂–∞–Ω–∏–µ, cost/time savings);\n"
        "- –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–ª–∞—Ç—Ñ–æ—Ä–º/—Ä–µ–≥—É–ª—è—Ç–æ—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä—è–º–æ –≤–ª–∏—è—é—Ç –Ω–∞ roadmap CoreAI.\n"
        "- –ù–ï high priority: ¬´–∫–æ–º–ø–∞–Ω–∏—è X –≤–Ω–µ–¥—Ä–∏–ª–∞ AI¬ª –±–µ–∑ —Ü–∏—Ñ—Ä; –∂–∞–ª–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ —Ç–æ–Ω/–ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–æ—Ç–∞; —Ä—É—Ç–∏–Ω–Ω—ã–π —Ä–µ–ª–∏–∑ –º–æ–¥–µ–ª–∏ (MiniMax, Llama –∏ —Ç.–¥.) ‚Äî —ç—Ç–æ tech_update —Å priority medium/low.\n\n"
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏ low priority:\n"
        "- –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö;\n"
        "- —Ä–µ–∫–ª–∞–º–∞, –≤–∞–∫–∞–Ω—Å–∏–∏, –∫—É—Ä—Å—ã, –æ–±—â–∏–µ IT-—Å—Ç–∞—Ç—å–∏ –±–µ–∑ AI-–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–≥–æ —É–≥–ª–∞;\n"
        "- –ª–æ–∫–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ —É—Ä–æ–∫–∞.\n\n"
        "–ß–¢–û –ù–ï –°–ß–ò–¢–ê–¢–¨ PRODUCT –ò –ù–ï –ê–õ–ï–†–¢–ò–¢–¨ (—Å—Ç—Ä–æ–≥–æ):\n"
        "- ¬´–ö–æ–º–ø–∞–Ω–∏—è X –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –Ω–µ –ø–∏—à—É—Ç –∫–æ–¥ –±–ª–∞–≥–æ–¥–∞—Ä—è AI¬ª / ¬´–≤–Ω–µ–¥—Ä–∏–ª–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ AI-—Ä–µ—à–µ–Ω–∏—è¬ª –±–µ–∑ —Ü–∏—Ñ—Ä (—ç–∫–æ–Ω–æ–º–∏—è %, —Å—Ä–æ–∫–∏, –æ–±—ä—ë–º) ‚Äî —ç—Ç–æ –Ω–µ product, —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π PR. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –∫–∞–∫ misc –∏–ª–∏ trend, priority=low, is_alert_worthy=false.\n"
        "- –ñ–∞–ª–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ —Ç–æ–Ω/–ø–æ–≤–µ–¥–µ–Ω–∏–µ ChatGPT/–¥—Ä—É–≥–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ (¬´–∫–æ–Ω–¥–µ–Ω—Å—Ü–µ–Ω–¥–∏—Ä—É—é—â–∏–µ –æ—Ç–≤–µ—Ç—ã¬ª, ¬´–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–æ—Ç–∏–≤—ã¬ª) ‚Äî —ç—Ç–æ –Ω–µ product, –Ω–µ –∫–µ–π—Å —É—Å–ø–µ—Ö–∞. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –∫–∞–∫ misc, priority=low, is_alert_worthy=false.\n"
        "- –†–µ–ª–∏–∑ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ (MiniMax M2.5, Llama, GLM –∏ —Ç.–¥.) ‚Äî –≤—Å–µ–≥–¥–∞ tech_update, –Ω–µ product. priority –¥–ª—è tech_update –º–æ–∂–µ—Ç –±—ã—Ç—å high —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ä—ã–Ω–∫–∞; –∏–Ω–∞—á–µ medium/low. –ù–µ —Ä–∞–∑–¥—É–≤–∞–π coreai_score –∏ is_alert_worthy –¥–ª—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö —Ä–µ–ª–∏–∑–æ–≤.\n"
        "- –õ—é–±–æ–µ ¬´–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ –Ω–µ—á—Ç–æ¬ª, –∫–æ—Ç–æ—Ä–æ–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å (–æ–±—â–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —á—É–∂–∏–µ –º–Ω–µ–Ω–∏—è –±–µ–∑ –∫–µ–π—Å–∞, –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ action_item) ‚Äî misc, priority=low, is_alert_worthy=false.\n\n"
        "–í–∞–∂–Ω–æ: (1) ¬´–ö–æ–º–ø–∞–Ω–∏—è X –≤—ã–ø—É—Å—Ç–∏–ª–∞/–∑–∞–ø—É—Å—Ç–∏–ª–∞ –º–æ–¥–µ–ª—å/–∞–≥–µ–Ω—Ç–∞/API¬ª ‚Äî –≤—Å–µ–≥–¥–∞ tech_update, –Ω–µ product. (2) product = —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –≤ –Ω–æ–≤–æ—Å—Ç–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è —É—Å–ø–µ—Ö–∞: —Ü–∏—Ñ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –≤—ã—Ä—É—á–∫–∞, —ç–∫–∑–∏—Ç, –∫–µ–π—Å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏. –ë–µ–∑ —ç—Ç–æ–≥–æ ‚Äî tech_update –∏–ª–∏ misc. (3) –í ¬´–í–∞–∂–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å¬ª –¥–ª—è CoreAI —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –≤–∞–∂–Ω–æ–µ: –∫–µ–π—Å—ã —É—Å–ø–µ—Ö–∞ –ø—Ä–æ–¥—É–∫—Ç–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–ª–∞—Ç—Ñ–æ—Ä–º/—Ä–µ–≥—É–ª—è—Ç–æ—Ä–∏–∫–∏. –ù–µ –ø–æ—Å—Ç–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ PR, –∂–∞–ª–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ä—É—Ç–∏–Ω–Ω—ã–µ —Ä–µ–ª–∏–∑—ã –º–æ–¥–µ–ª–µ–π.\n\n"
        "–ü—Ä–∞–≤–∏–ª–æ –¥–ª—è is_alert_worthy=true:\n"
        "- —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å high priority –ò–õ–ò coreai_score >= 0.78;\n"
        "- –¥–ª—è product: —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ü–∏—Ñ—Ä —É—Å–ø–µ—Ö–∞ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –≤—ã—Ä—É—á–∫–∞, —ç–∫–∑–∏—Ç, –∫–µ–π—Å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏). –ë–µ–∑ —Ü–∏—Ñ—Ä ‚Äî is_alert_worthy=false.\n"
        "- –¥–ª—è tech_update: true —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ–º —Ä–µ–ª–∏–∑–µ/–∏–∑–º–µ–Ω–µ–Ω–∏–∏, –Ω–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–Ω–æ–Ω—Å–∞ –º–æ–¥–µ–ª–∏.\n"
        "- –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π PR (¬´–∫–æ–º–ø–∞–Ω–∏—è –≤–Ω–µ–¥—Ä–∏–ª–∞ AI¬ª –±–µ–∑ –º–µ—Ç—Ä–∏–∫), –∂–∞–ª–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è ‚Äî –≤—Å–µ–≥–¥–∞ is_alert_worthy=false.\n"
        "- —É –Ω–æ–≤–æ—Å—Ç–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–µ—Ç–∫–∏–π, —Ä–µ–∞–ª–∏–∑—É–µ–º—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è CoreAI.\n\n"
        "–ü—Ä–∞–≤–∏–ª–æ –¥–ª—è analogs:\n"
        "- –≤–∫–ª—é—á–∞–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã/–∫–æ–º–ø–∞–Ω–∏–∏;\n"
        "- –Ω–µ –±–æ–ª–µ–µ 3;\n"
        "- –µ—Å–ª–∏ –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤.\n\n"
        "–ü—Ä–∞–≤–∏–ª–æ –¥–ª—è action_item:\n"
        "- –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞ 1-2 –Ω–µ–¥–µ–ª–∏;\n"
        "- –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä: —Å—Ä–∞–≤–Ω–∏—Ç—å X —Å –Ω–∞—à–∏–º Y –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–∏–ª–æ—Ç –Ω–∞ Z).\n\n"
        "–ù–∏–∫–∞–∫–æ–≥–æ markdown. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON."
    )

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": content},
            ],
            max_tokens=450,
            temperature=0.1,
        )
        raw = response.choices[0].message.content.strip()
        data = _extract_json_object(raw)

        summary = str(data.get("summary", "")).strip()
        if not summary:
            summary = content[:300] + "..." if len(content) > 300 else content

        raw_relevant = data.get("is_relevant", False)
        if isinstance(raw_relevant, bool):
            is_relevant = raw_relevant
        else:
            is_relevant = str(raw_relevant).strip().lower() in {"true", "yes", "1"}
        try:
            coreai_score = float(data.get("coreai_score", 0.0))
        except Exception:
            coreai_score = 0.0
        coreai_score = max(0.0, min(1.0, coreai_score))

        coreai_reason = str(data.get("coreai_reason", "")).strip()
        news_kind = str(data.get("news_kind", "misc")).strip().lower()
        if news_kind not in {"product", "trend", "research", "tech_update", "industry_report", "misc"}:
            news_kind = "misc"
        raw_impl = data.get("implementable_by_small_team", False)
        if isinstance(raw_impl, bool):
            implementable_by_small_team = raw_impl
        else:
            implementable_by_small_team = str(raw_impl).strip().lower() in {"true", "yes", "1"}
        infra_barrier = str(data.get("infra_barrier", "high")).strip().lower()
        if infra_barrier not in {"low", "medium", "high"}:
            infra_barrier = "high"
        try:
            product_score = float(data.get("product_score", 0.0))
        except Exception:
            product_score = 0.0
        product_score = max(0.0, min(1.0, product_score))
        priority = str(data.get("priority", "low")).strip().lower()
        if priority not in {"high", "medium", "low"}:
            priority = "low"
        raw_alert = data.get("is_alert_worthy", False)
        if isinstance(raw_alert, bool):
            is_alert_worthy = raw_alert
        else:
            is_alert_worthy = str(raw_alert).strip().lower() in {"true", "yes", "1"}
        raw_analogs = data.get("analogs", [])
        if isinstance(raw_analogs, str):
            raw_analogs = [a.strip() for a in raw_analogs.split(",") if a.strip()]
        elif not isinstance(raw_analogs, list):
            raw_analogs = []
        analogs = [str(a).strip() for a in raw_analogs if str(a).strip()][:3]
        action_item = str(data.get("action_item", "")).strip()
        if not action_item and news_kind == "product":
            action_item = "–°—Ä–∞–≤–Ω–∏—Ç—å —Ñ–∏—á—É —Å –Ω–∞—à–∏–º roadmap –∏ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç."
        raw_tags = data.get("tags", [])
        if isinstance(raw_tags, str):
            raw_tags = [tag.strip() for tag in raw_tags.split(",") if tag.strip()]
        elif not isinstance(raw_tags, list):
            raw_tags = []
        allowed = set(NEWS_TAGS)
        tags = [tag for tag in raw_tags if isinstance(tag, str) and tag in allowed]
        if not tags and is_relevant:
            tags = ["#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
        tags = tags[:3]
        return {
            "summary": summary,
            "is_relevant": is_relevant,
            "coreai_score": coreai_score,
            "coreai_reason": coreai_reason,
            "tags": tags,
            "news_kind": news_kind,
            "implementable_by_small_team": implementable_by_small_team,
            "infra_barrier": infra_barrier,
            "product_score": product_score,
            "priority": priority,
            "is_alert_worthy": is_alert_worthy,
            "analogs": analogs,
            "action_item": action_item,
        }
    except Exception as e:
        logger.error(f"LLM combined analysis error: {e}")
        fallback_summary = content[:300] + "..." if len(content) > 300 else content
        return {
            "summary": fallback_summary,
            "is_relevant": True,
            "coreai_score": 0.0,
            "coreai_reason": "LLM error fallback",
            "tags": ["#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"],
            "news_kind": "misc",
            "implementable_by_small_team": False,
            "infra_barrier": "high",
            "product_score": 0.0,
            "priority": "low",
            "is_alert_worthy": False,
            "analogs": [],
            "action_item": "",
        }


async def score_user_prompt_relevance(summary: str, user_prompt: str) -> float:
    """Score how relevant a news summary is to the user custom prompt (0..1)."""
    if not user_prompt or len(user_prompt.strip()) < 5:
        return 0.5

    client = get_llm_client()
    prompt = (
        "–¢—ã —Ñ–∏–ª—å—Ç—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π.\n"
        "–û—Ü–µ–Ω–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É —Ñ–∏–ª—å—Ç—Ä—É.\n"
        "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON: {\"user_relevance_score\": number}\n"
        "–ì–¥–µ score –æ—Ç 0 –¥–æ 1.\n"
        "0 = –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç, 1 = –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç.\n"
        "–ë–µ–∑ markdown –∏ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞."
    )
    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {"role": "system", "content": prompt},
                {
                    "role": "user",
                    "content": f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä:\n{user_prompt[:1200]}\n\n–ù–æ–≤–æ—Å—Ç—å:\n{summary[:1500]}",
                },
            ],
            max_tokens=80,
            temperature=0,
        )
        raw = response.choices[0].message.content.strip()
        data = _extract_json_object(raw)
        score = float(data.get("user_relevance_score", 0.5))
        return max(0.0, min(1.0, score))
    except Exception as e:
        logger.debug(f"User relevance scoring error: {e}")
        return 0.5


async def check_ai_relevance(text: str) -> bool:
    """
    Check if a post is a real AI/ML/tech news (not an ad, promo, or off-topic).
    Returns True if the post is AI-relevant news, False otherwise.
    """
    client = get_llm_client()

    # Truncate to keep it cheap and fast
    text = text[:500]

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî —Ñ–∏–ª—å—Ç—Ä –Ω–æ–≤–æ—Å—Ç–µ–π. –û–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –†–ï–ê–õ–¨–ù–û–ô –Ω–æ–≤–æ—Å—Ç—å—é –∏–ª–∏ —Å—Ç–∞—Ç—å—ë–π "
                        "–ø—Ä–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, LLM, GPT, "
                        "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é –ò–ò –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.\n\n"
                        "–û—Ç–≤–µ—Ç—å –°–¢–†–û–ì–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:\n"
                        "YES ‚Äî –µ—Å–ª–∏ —ç—Ç–æ –Ω–∞—Å—Ç–æ—è—â–∞—è –Ω–æ–≤–æ—Å—Ç—å/—Å—Ç–∞—Ç—å—è –ø—Ä–æ –ò–ò/ML/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏\n"
                        "NO ‚Äî –µ—Å–ª–∏ —ç—Ç–æ —Ä–µ–∫–ª–∞–º–∞, –ø—Ä–æ–º–æ, –ø—Ä–æ–¥–∞–∂–∞ –∫—É—Ä—Å–æ–≤, –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –ø–ª–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, "
                        "–ª–∏—á–Ω–æ–µ –º–Ω–µ–Ω–∏–µ –±–µ–∑ –Ω–æ–≤–æ—Å—Ç–∏, —Å–ø–∞–º, –∏–ª–∏ —Ç–µ–º–∞ –ù–ï —Å–≤—è–∑–∞–Ω–∞ —Å –ò–ò\n\n"
                        "–ü—Ä–∏–º–µ—Ä—ã NO: –ø—Ä–æ–¥–∞–∂–∞ –∫—É—Ä—Å–æ–≤, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∫–∏, —Ä–æ–∑—ã–≥—Ä—ã—à, "
                        "–ø—Ä–æ–º–æ–∫–æ–¥, –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∞—è —Å—Å—ã–ª–∫–∞, –Ω–∞–±–æ—Ä –Ω–∞ –≤–µ–±–∏–Ω–∞—Ä, –≤–∞–∫–∞–Ω—Å–∏—è."
                    ),
                },
                {
                    "role": "user",
                    "content": text,
                },
            ],
            max_tokens=5,
            temperature=0,
        )
        answer = response.choices[0].message.content.strip().upper()
        is_relevant = answer.startswith("YES")
        logger.debug(f"AI relevance check: '{text[:60]}...' -> {answer} ({is_relevant})")
        return is_relevant
    except Exception as e:
        logger.error(f"AI relevance check error: {e}")
        # Default to True on error ‚Äî don't lose real news
        return True


async def check_similarity(post1_summary: str, post2_summary: str) -> dict:
    """
    Ask LLM to confirm whether two posts are about the same news event.
    Returns {"is_similar": bool, "explanation": str}
    """
    client = get_llm_client()

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—à—å –¥–≤–µ –Ω–æ–≤–æ—Å—Ç–∏. –û–ø—Ä–µ–¥–µ–ª–∏, –æ–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ –æ–Ω–∏ –û–î–ù–û –ò –¢–û –ñ–ï —Å–æ–±—ã—Ç–∏–µ. "
                        "–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
                        "SIMILAR: –¥–∞/–Ω–µ—Ç\n"
                        "–ü–†–ò–ß–ò–ù–ê: <–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º>\n"
                        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"–ù–æ–≤–æ—Å—Ç—å 1:\n{post1_summary}\n\n"
                        f"–ù–æ–≤–æ—Å—Ç—å 2:\n{post2_summary}\n\n"
                        "–≠—Ç–æ –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –Ω–æ–≤–æ—Å—Ç—å?"
                    ),
                },
            ],
            max_tokens=200,
            temperature=0.1,
        )
        text = response.choices[0].message.content.strip()

        is_similar = "–¥–∞" in text.lower().split("\n")[0]
        # Extract explanation
        explanation = ""
        for line in text.split("\n"):
            if line.upper().startswith("–ü–†–ò–ß–ò–ù–ê:"):
                explanation = line.split(":", 1)[1].strip()
                break

        if not explanation:
            explanation = text

        return {"is_similar": is_similar, "explanation": explanation}

    except Exception as e:
        logger.error(f"LLM similarity check error: {e}")
        return {"is_similar": False, "explanation": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"}


async def generate_digest_text(summaries: list[dict]) -> Optional[str]:
    """
    Generate a formatted daily digest from a list of post summaries.
    summaries: list of {"source": str, "summary": str, "reactions": int, "tags": str, "mentions": int}
    """
    client = get_llm_client()

    if not summaries:
        return None

    posts_text = "\n\n".join(
        f"[{s['source']}] (—Ä–µ–∞–∫—Ü–∏–π: {s['reactions']}, –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {s.get('mentions', 1)})\n"
        f"–¢–µ–≥–∏: {s.get('tags') or '#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'}\n"
        f"{s['summary']}"
        for s in summaries
    )

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è Telegram. "
                        "–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ—Å—Ç–µ–π. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –≤–º–µ—Å—Ç–µ.\n"
                        "–§–û–ö–£–°: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ AI/LLM (—Ä–µ–ª–∏–∑—ã, –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Ñ–∏—á–∏, API).\n"
                        "–¢—Ä–µ–Ω–¥—ã –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–∞–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–Ω–∞—á–∏–º—ã.\n\n"
                        "–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø:\n"
                        "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ HTML-—Ç–µ–≥–∏: <b>–∂–∏—Ä–Ω—ã–π</b>, <i>–∫—É—Ä—Å–∏–≤</i>\n"
                        "- –î–ª—è —Å–ø–∏—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–∏—Ä–µ: - —Ç–µ–∫—Å—Ç\n"
                        "- –î–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å —Å—Ç—Ä–æ–∫—É: –¢–µ–≥–∏: #tag1 #tag2\n"
                        "- –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π * –∏–ª–∏ ** –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n"
                        "- –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown-—Å–∏–Ω—Ç–∞–∫—Å–∏—Å\n\n"
                        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n"
                        "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –∑–∞ —Å–µ–≥–æ–¥–Ω—è</b>\n\n"
                        "üî• <b>–ì–ª–∞–≤–Ω–æ–µ:</b>\n"
                        "- <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫.</b> –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n\n"
                        "üìå <b>–¢–∞–∫–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:</b>\n"
                        "- <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫.</b> –û–ø–∏—Å–∞–Ω–∏–µ.\n\n"
                        "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
                    ),
                },
                {
                    "role": "user",
                    "content": f"–í–æ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è:\n\n{posts_text}",
                },
            ],
            max_tokens=1100,
            temperature=0.5,
        )
        digest = response.choices[0].message.content.strip()
        return digest
    except Exception as e:
        logger.error(f"LLM digest generation error: {e}")
        return None


async def analyze_business_impact(summary: str, contexts: list[dict]) -> dict:
    """
    Analyze real business impact with positive and negative precedents.
    contexts: list of {"title": str, "snippet": str, "url": str}
    Returns:
      {
        "impact_score": float 0..1,
        "positive_precedents": list[str],
        "negative_precedents": list[str],
        "conclusion": str,
      }
    """
    client = get_llm_client()

    context_text = "\n\n".join(
        f"[{c.get('title', 'source')}]\n{c.get('snippet', '')}\nURL: {c.get('url', '')}"
        for c in contexts[:8]
    )

    prompt = (
        "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤–ª–∏—è–Ω–∏—è AI-–Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å.\n"
        "–ù—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏–∏, –∫–æ–º–∞–Ω–¥—ã –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.\n"
        "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON —Å –ø–æ–ª—è–º–∏:\n"
        "impact_score: number –æ—Ç 0 –¥–æ 1\n"
        "positive_precedents: array<string> (1-3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—É–Ω–∫—Ç–∞)\n"
        "negative_precedents: array<string> (1-3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—É–Ω–∫—Ç–∞)\n"
        "conclusion: string (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n"
        "–û—Ü–µ–Ω–∏–≤–∞–π –≤—ã—à–µ, –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–µ–π—Å—ã –≤–Ω–µ–¥—Ä–µ–Ω–∏—è, –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –≤—ã—Ä—É—á–∫—É/–∏–∑–¥–µ—Ä–∂–∫–∏/—Ä–∏—Å–∫.\n"
        "–ù–∏–∫–∞–∫–æ–≥–æ markdown –∏ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON."
    )

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {"role": "system", "content": prompt},
                {
                    "role": "user",
                    "content": (
                        f"–ù–æ–≤–æ—Å—Ç—å:\n{summary[:1200]}\n\n"
                        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:\n{context_text[:5000]}"
                    ),
                },
            ],
            max_tokens=400,
            temperature=0.1,
        )
        raw = response.choices[0].message.content.strip()
        data = _extract_json_object(raw)

        try:
            impact_score = float(data.get("impact_score", 0.0))
        except Exception:
            impact_score = 0.0
        impact_score = max(0.0, min(1.0, impact_score))

        positive = data.get("positive_precedents", [])
        if not isinstance(positive, list):
            positive = []
        positive = [str(x).strip() for x in positive if str(x).strip()][:3]

        negative = data.get("negative_precedents", [])
        if not isinstance(negative, list):
            negative = []
        negative = [str(x).strip() for x in negative if str(x).strip()][:3]

        conclusion = str(data.get("conclusion", "")).strip()
        return {
            "impact_score": impact_score,
            "positive_precedents": positive,
            "negative_precedents": negative,
            "conclusion": conclusion,
        }
    except Exception as e:
        logger.error(f"LLM business impact analysis error: {e}")
        return {
            "impact_score": 0.0,
            "positive_precedents": [],
            "negative_precedents": [],
            "conclusion": "",
        }
