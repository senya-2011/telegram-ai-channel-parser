import logging
import json
import re
from typing import Optional

from openai import AsyncOpenAI

from app.config import settings

logger = logging.getLogger(__name__)

_client: AsyncOpenAI | None = None

NEWS_TAGS = [
    "#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    "#LLM",
    "#GPT",
    "#–ê–≥–µ–Ω—Ç—ã",
    "#OpenSourceAI",
    "#–†–µ–ª–∏–∑–ú–æ–¥–µ–ª–∏",
    "#–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏AI",
    "#–†–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µAI",
    "#–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—åAI",
    "#–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
]


def _extract_json_object(text: str) -> dict:
    text = text.strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if not match:
            raise
        return json.loads(match.group(0))


def get_llm_client() -> AsyncOpenAI:
    global _client
    if _client is None:
        _client = AsyncOpenAI(
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url,
        )
    return _client


async def summarize_post(content: str) -> Optional[str]:
    """Generate a concise summary of a post/article using DeepSeek."""
    client = get_llm_client()

    # Truncate very long content
    if len(content) > 4000:
        content = content[:4000] + "..."

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫—Ä–∞—Ç–∫–æ –∏–∑–ª–æ–∂–∏—Ç—å —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ "
                        "–≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –≤—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã. "
                        "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Å–≤–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏."
                    ),
                },
                {
                    "role": "user",
                    "content": f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏:\n\n{content}",
                },
            ],
            max_tokens=300,
            temperature=0.3,
        )
        summary = response.choices[0].message.content.strip()
        logger.debug(f"Summary generated: {summary[:80]}...")
        return summary
    except Exception as e:
        logger.error(f"LLM summarization error: {e}")
        return None


async def analyze_post(content: str) -> dict:
    """
    Single LLM call for summary + AI relevance + CoreAI importance.
    Returns:
      {
        "summary": str,
        "is_relevant": bool,
        "coreai_score": float (0..1),
        "coreai_reason": str,
        "tags": list[str],
      }
    """
    client = get_llm_client()

    if len(content) > 4000:
        content = content[:4000] + "..."

    prompt = (
        "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ CoreAI. –ù—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ü–µ–Ω–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ AI-–∏–Ω–¥—É—Å—Ç—Ä–∏–∏.\n"
        "–í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –°–¢–†–û–ì–û –≤ JSON —Å –ø–æ–ª—è–º–∏:\n"
        "summary: string (–∫—Ä–∞—Ç–∫–æ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º)\n"
        "is_relevant: boolean (true —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏/—Å—Ç–∞—Ç—å–∏ –ø–æ AI/ML/LLM)\n"
        "coreai_score: number –æ—Ç 0 –¥–æ 1\n"
        "coreai_reason: string (–ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ/–Ω–µ –≤–∞–∂–Ω–æ –¥–ª—è CoreAI)\n"
        "tags: array<string> –∏–∑ 1-3 —Ö–µ—à—Ç–µ–≥–æ–≤\n"
        f"–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —ç—Ç–∏ —Ö–µ—à—Ç–µ–≥–∏: {', '.join(NEWS_TAGS)}\n"
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è CoreAI:\n"
        "- —Ä–µ–ª–∏–∑/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–ª–∏ API —É –∫–ª—é—á–µ–≤—ã—Ö –∏–≥—Ä–æ–∫–æ–≤ (OpenAI, Anthropic, Google, Meta, xAI, Mistral, DeepSeek)\n"
        "- –Ω–æ–≤—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, SOTA-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–∞—É—á–Ω—ã–µ –ø—Ä–æ—Ä—ã–≤—ã\n"
        "- –∫—Ä—É–ø–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, M&A, –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞ –∏ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ AI\n"
        "- –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞/–∞–≥–µ–Ω—Ç—ã/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –±–∏–∑–Ω–µ—Å\n"
        "–ù–∏–∑–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å:\n"
        "- —Ä–µ–∫–ª–∞–º–∞ –∫—É—Ä—Å–æ–≤, –≤–∞–∫–∞–Ω—Å–∏–∏, –ø—Ä–æ–º–æ, –ª–∏—á–Ω—ã–µ –º–Ω–µ–Ω–∏—è –±–µ–∑ —Ñ–∞–∫—Ç–æ–≤\n"
        "- —Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –±–µ–∑ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏\n"
        "–ù–∏–∫–∞–∫–æ–≥–æ markdown, —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON."
    )

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": content},
            ],
            max_tokens=450,
            temperature=0.1,
        )
        raw = response.choices[0].message.content.strip()
        data = _extract_json_object(raw)

        summary = str(data.get("summary", "")).strip()
        if not summary:
            summary = content[:300] + "..." if len(content) > 300 else content

        raw_relevant = data.get("is_relevant", False)
        if isinstance(raw_relevant, bool):
            is_relevant = raw_relevant
        else:
            is_relevant = str(raw_relevant).strip().lower() in {"true", "yes", "1"}
        try:
            coreai_score = float(data.get("coreai_score", 0.0))
        except Exception:
            coreai_score = 0.0
        coreai_score = max(0.0, min(1.0, coreai_score))

        coreai_reason = str(data.get("coreai_reason", "")).strip()
        raw_tags = data.get("tags", [])
        if isinstance(raw_tags, str):
            raw_tags = [tag.strip() for tag in raw_tags.split(",") if tag.strip()]
        elif not isinstance(raw_tags, list):
            raw_tags = []
        allowed = set(NEWS_TAGS)
        tags = [tag for tag in raw_tags if isinstance(tag, str) and tag in allowed]
        if not tags and is_relevant:
            tags = ["#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
        tags = tags[:3]
        return {
            "summary": summary,
            "is_relevant": is_relevant,
            "coreai_score": coreai_score,
            "coreai_reason": coreai_reason,
            "tags": tags,
        }
    except Exception as e:
        logger.error(f"LLM combined analysis error: {e}")
        fallback_summary = content[:300] + "..." if len(content) > 300 else content
        return {
            "summary": fallback_summary,
            "is_relevant": True,
            "coreai_score": 0.0,
            "coreai_reason": "LLM error fallback",
            "tags": ["#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"],
        }


async def check_ai_relevance(text: str) -> bool:
    """
    Check if a post is a real AI/ML/tech news (not an ad, promo, or off-topic).
    Returns True if the post is AI-relevant news, False otherwise.
    """
    client = get_llm_client()

    # Truncate to keep it cheap and fast
    text = text[:500]

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî —Ñ–∏–ª—å—Ç—Ä –Ω–æ–≤–æ—Å—Ç–µ–π. –û–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –†–ï–ê–õ–¨–ù–û–ô –Ω–æ–≤–æ—Å—Ç—å—é –∏–ª–∏ —Å—Ç–∞—Ç—å—ë–π "
                        "–ø—Ä–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, LLM, GPT, "
                        "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é –ò–ò –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.\n\n"
                        "–û—Ç–≤–µ—Ç—å –°–¢–†–û–ì–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º:\n"
                        "YES ‚Äî –µ—Å–ª–∏ —ç—Ç–æ –Ω–∞—Å—Ç–æ—è—â–∞—è –Ω–æ–≤–æ—Å—Ç—å/—Å—Ç–∞—Ç—å—è –ø—Ä–æ –ò–ò/ML/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏\n"
                        "NO ‚Äî –µ—Å–ª–∏ —ç—Ç–æ —Ä–µ–∫–ª–∞–º–∞, –ø—Ä–æ–º–æ, –ø—Ä–æ–¥–∞–∂–∞ –∫—É—Ä—Å–æ–≤, –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –ø–ª–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, "
                        "–ª–∏—á–Ω–æ–µ –º–Ω–µ–Ω–∏–µ –±–µ–∑ –Ω–æ–≤–æ—Å—Ç–∏, —Å–ø–∞–º, –∏–ª–∏ —Ç–µ–º–∞ –ù–ï —Å–≤—è–∑–∞–Ω–∞ —Å –ò–ò\n\n"
                        "–ü—Ä–∏–º–µ—Ä—ã NO: –ø—Ä–æ–¥–∞–∂–∞ –∫—É—Ä—Å–æ–≤, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∫–∏, —Ä–æ–∑—ã–≥—Ä—ã—à, "
                        "–ø—Ä–æ–º–æ–∫–æ–¥, –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∞—è —Å—Å—ã–ª–∫–∞, –Ω–∞–±–æ—Ä –Ω–∞ –≤–µ–±–∏–Ω–∞—Ä, –≤–∞–∫–∞–Ω—Å–∏—è."
                    ),
                },
                {
                    "role": "user",
                    "content": text,
                },
            ],
            max_tokens=5,
            temperature=0,
        )
        answer = response.choices[0].message.content.strip().upper()
        is_relevant = answer.startswith("YES")
        logger.debug(f"AI relevance check: '{text[:60]}...' -> {answer} ({is_relevant})")
        return is_relevant
    except Exception as e:
        logger.error(f"AI relevance check error: {e}")
        # Default to True on error ‚Äî don't lose real news
        return True


async def check_similarity(post1_summary: str, post2_summary: str) -> dict:
    """
    Ask LLM to confirm whether two posts are about the same news event.
    Returns {"is_similar": bool, "explanation": str}
    """
    client = get_llm_client()

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—à—å –¥–≤–µ –Ω–æ–≤–æ—Å—Ç–∏. –û–ø—Ä–µ–¥–µ–ª–∏, –æ–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ –æ–Ω–∏ –û–î–ù–û –ò –¢–û –ñ–ï —Å–æ–±—ã—Ç–∏–µ. "
                        "–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
                        "SIMILAR: –¥–∞/–Ω–µ—Ç\n"
                        "–ü–†–ò–ß–ò–ù–ê: <–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º>\n"
                        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"–ù–æ–≤–æ—Å—Ç—å 1:\n{post1_summary}\n\n"
                        f"–ù–æ–≤–æ—Å—Ç—å 2:\n{post2_summary}\n\n"
                        "–≠—Ç–æ –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –Ω–æ–≤–æ—Å—Ç—å?"
                    ),
                },
            ],
            max_tokens=200,
            temperature=0.1,
        )
        text = response.choices[0].message.content.strip()

        is_similar = "–¥–∞" in text.lower().split("\n")[0]
        # Extract explanation
        explanation = ""
        for line in text.split("\n"):
            if line.upper().startswith("–ü–†–ò–ß–ò–ù–ê:"):
                explanation = line.split(":", 1)[1].strip()
                break

        if not explanation:
            explanation = text

        return {"is_similar": is_similar, "explanation": explanation}

    except Exception as e:
        logger.error(f"LLM similarity check error: {e}")
        return {"is_similar": False, "explanation": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"}


async def generate_digest_text(summaries: list[dict]) -> Optional[str]:
    """
    Generate a formatted daily digest from a list of post summaries.
    summaries: list of {"source": str, "summary": str, "reactions": int, "tags": str, "mentions": int}
    """
    client = get_llm_client()

    if not summaries:
        return None

    posts_text = "\n\n".join(
        f"[{s['source']}] (—Ä–µ–∞–∫—Ü–∏–π: {s['reactions']}, –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {s.get('mentions', 1)})\n"
        f"–¢–µ–≥–∏: {s.get('tags') or '#AI–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'}\n"
        f"{s['summary']}"
        for s in summaries
    )

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è Telegram. "
                        "–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ—Å—Ç–µ–π. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –≤–º–µ—Å—Ç–µ.\n\n"
                        "–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø:\n"
                        "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ HTML-—Ç–µ–≥–∏: <b>–∂–∏—Ä–Ω—ã–π</b>, <i>–∫—É—Ä—Å–∏–≤</i>\n"
                        "- –î–ª—è —Å–ø–∏—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–∏—Ä–µ: - —Ç–µ–∫—Å—Ç\n"
                        "- –î–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å —Å—Ç—Ä–æ–∫—É: –¢–µ–≥–∏: #tag1 #tag2\n"
                        "- –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π * –∏–ª–∏ ** –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n"
                        "- –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown-—Å–∏–Ω—Ç–∞–∫—Å–∏—Å\n\n"
                        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n"
                        "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –∑–∞ —Å–µ–≥–æ–¥–Ω—è</b>\n\n"
                        "üî• <b>–ì–ª–∞–≤–Ω–æ–µ:</b>\n"
                        "- <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫.</b> –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n\n"
                        "üìå <b>–¢–∞–∫–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:</b>\n"
                        "- <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫.</b> –û–ø–∏—Å–∞–Ω–∏–µ.\n\n"
                        "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
                    ),
                },
                {
                    "role": "user",
                    "content": f"–í–æ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è:\n\n{posts_text}",
                },
            ],
            max_tokens=1500,
            temperature=0.5,
        )
        digest = response.choices[0].message.content.strip()
        return digest
    except Exception as e:
        logger.error(f"LLM digest generation error: {e}")
        return None
