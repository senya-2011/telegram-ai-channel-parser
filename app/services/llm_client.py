import logging
from typing import Optional

from openai import AsyncOpenAI

from app.config import settings

logger = logging.getLogger(__name__)

_client: AsyncOpenAI | None = None


def get_llm_client() -> AsyncOpenAI:
    global _client
    if _client is None:
        _client = AsyncOpenAI(
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url,
        )
    return _client


async def summarize_post(content: str) -> Optional[str]:
    """Generate a concise summary of a post/article using DeepSeek."""
    client = get_llm_client()

    # Truncate very long content
    if len(content) > 4000:
        content = content[:4000] + "..."

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫—Ä–∞—Ç–∫–æ –∏–∑–ª–æ–∂–∏—Ç—å —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ "
                        "–≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –≤—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã. "
                        "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Å–≤–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –æ—Ü–µ–Ω–∫–∏."
                    ),
                },
                {
                    "role": "user",
                    "content": f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏:\n\n{content}",
                },
            ],
            max_tokens=300,
            temperature=0.3,
        )
        summary = response.choices[0].message.content.strip()
        logger.debug(f"Summary generated: {summary[:80]}...")
        return summary
    except Exception as e:
        logger.error(f"LLM summarization error: {e}")
        return None


async def check_similarity(post1_summary: str, post2_summary: str) -> dict:
    """
    Ask LLM to confirm whether two posts are about the same news event.
    Returns {"is_similar": bool, "explanation": str}
    """
    client = get_llm_client()

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—à—å –¥–≤–µ –Ω–æ–≤–æ—Å—Ç–∏. –û–ø—Ä–µ–¥–µ–ª–∏, –æ–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ –æ–Ω–∏ –û–î–ù–û –ò –¢–û –ñ–ï —Å–æ–±—ã—Ç–∏–µ. "
                        "–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
                        "SIMILAR: –¥–∞/–Ω–µ—Ç\n"
                        "–ü–†–ò–ß–ò–ù–ê: <–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º>\n"
                        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"–ù–æ–≤–æ—Å—Ç—å 1:\n{post1_summary}\n\n"
                        f"–ù–æ–≤–æ—Å—Ç—å 2:\n{post2_summary}\n\n"
                        "–≠—Ç–æ –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –Ω–æ–≤–æ—Å—Ç—å?"
                    ),
                },
            ],
            max_tokens=200,
            temperature=0.1,
        )
        text = response.choices[0].message.content.strip()

        is_similar = "–¥–∞" in text.lower().split("\n")[0]
        # Extract explanation
        explanation = ""
        for line in text.split("\n"):
            if line.upper().startswith("–ü–†–ò–ß–ò–ù–ê:"):
                explanation = line.split(":", 1)[1].strip()
                break

        if not explanation:
            explanation = text

        return {"is_similar": is_similar, "explanation": explanation}

    except Exception as e:
        logger.error(f"LLM similarity check error: {e}")
        return {"is_similar": False, "explanation": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"}


async def generate_digest_text(summaries: list[dict]) -> Optional[str]:
    """
    Generate a formatted daily digest from a list of post summaries.
    summaries: list of {"source": str, "summary": str, "reactions": int}
    """
    client = get_llm_client()

    if not summaries:
        return None

    posts_text = "\n\n".join(
        f"[{s['source']}] (—Ä–µ–∞–∫—Ü–∏–π: {s['reactions']})\n{s['summary']}"
        for s in summaries
    )

    try:
        response = await client.chat.completions.create(
            model=settings.deepseek_model,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è Telegram. "
                        "–°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ—Å—Ç–µ–π. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –≤–º–µ—Å—Ç–µ.\n\n"
                        "–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø:\n"
                        "- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —ç—Ç–∏ HTML-—Ç–µ–≥–∏: <b>–∂–∏—Ä–Ω—ã–π</b>, <i>–∫—É—Ä—Å–∏–≤</i>\n"
                        "- –î–ª—è —Å–ø–∏—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–∏—Ä–µ: - —Ç–µ–∫—Å—Ç\n"
                        "- –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π * –∏–ª–∏ ** –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n"
                        "- –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown-—Å–∏–Ω—Ç–∞–∫—Å–∏—Å\n\n"
                        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n"
                        "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –∑–∞ —Å–µ–≥–æ–¥–Ω—è</b>\n\n"
                        "üî• <b>–ì–ª–∞–≤–Ω–æ–µ:</b>\n"
                        "- <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫.</b> –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n\n"
                        "üìå <b>–¢–∞–∫–∂–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ:</b>\n"
                        "- <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫.</b> –û–ø–∏—Å–∞–Ω–∏–µ.\n\n"
                        "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
                    ),
                },
                {
                    "role": "user",
                    "content": f"–í–æ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è:\n\n{posts_text}",
                },
            ],
            max_tokens=1500,
            temperature=0.5,
        )
        digest = response.choices[0].message.content.strip()
        return digest
    except Exception as e:
        logger.error(f"LLM digest generation error: {e}")
        return None
